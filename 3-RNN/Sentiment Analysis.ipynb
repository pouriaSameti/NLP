{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:18:29.714703400Z",
     "start_time": "2024-06-01T14:18:29.706653200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pouri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "import gensim.downloader as api\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:18:31.061946500Z",
     "start_time": "2024-06-01T14:18:30.125771Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      text  \\\n0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n1        is upset that he can't update his Facebook by ...   \n2        @Kenichan I dived many times for the ball. Man...   \n3          my whole body feels itchy and like its on fire    \n4        @nationwideclass no, it's not behaving at all....   \n...                                                    ...   \n1599995  Just woke up. Having no school is the best fee...   \n1599996  TheWDB.com - Very cool to hear old Walt interv...   \n1599997  Are you ready for your MoJo Makeover? Ask me f...   \n1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n\n                                 date             user  sentiment     query  \n0        Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n1        Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n2        Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n3        Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n4        Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n...                               ...              ...        ...       ...  \n1599995  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028          4  NO_QUERY  \n1599996  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards          4  NO_QUERY  \n1599997  Tue Jun 16 08:40:49 PDT 2009           bpbabe          4  NO_QUERY  \n1599998  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz          4  NO_QUERY  \n1599999  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris          4  NO_QUERY  \n\n[1600000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>date</th>\n      <th>user</th>\n      <th>sentiment</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>_TheSpecialOne_</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is upset that he can't update his Facebook by ...</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>scotthamilton</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>mattycus</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>ElleCTF</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>Karoli</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>Just woke up. Having no school is the best fee...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>AmandaMarie1028</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>TheWDBoards</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>bpbabe</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>tinydiamondz</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>RyanTrevMorris</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data/sentiment140.csv')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:18:36.376502Z",
     "start_time": "2024-06-01T14:18:32.240668700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "#remove web addresses, signs\n",
    "#change to lowercase\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_en_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub('@\\S+', '<MENTION>', text)\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "    # removing sign\n",
    "    text = ''.join([i for i in text if ord(i) not in [33, 34, 35, 36, 37, 38,\n",
    "                                                      39, 40, 41, 42, 43, 44,\n",
    "                                                      45, 46, 47, 58, 59, 60,\n",
    "                                                      61, 62, 63, 64, 91, 92,\n",
    "                                                      93, 94, 95, 96, 123, 124,\n",
    "                                                      125, 126, 1548, 1567]])\n",
    "    return text\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(clean_en_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:19:51.727994Z",
     "start_time": "2024-06-01T14:18:38.189954400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "dataset['text'] = dataset['text'].apply(lambda x: tokenizer.tokenize(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:20:06.564408900Z",
     "start_time": "2024-06-01T14:19:51.857091500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizer(text_list):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    words = [lemm.lemmatize(word) for word in text_list]\n",
    "    return words\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T12:33:02.302588200Z",
     "start_time": "2024-06-01T12:32:08.028043400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0     [MENTION, URL, awww, that, s, a, bummer, you, ...\n1     [is, upset, that, he, can, t, update, his, fac...\n2     [MENTION, i, dived, many, times, for, the, bal...\n3     [my, whole, body, feels, itchy, and, like, its...\n4     [MENTION, no, it, s, not, behaving, at, all, i...\n5                      [MENTION, not, the, whole, crew]\n6                                        [need, a, hug]\n7     [MENTION, hey, long, time, no, see, yes, rains...\n8              [MENTION, nope, they, didn, t, have, it]\n9                             [MENTION, que, me, muera]\n10     [spring, break, in, plain, city, it, s, snowing]\n11                     [i, just, re, pierced, my, ears]\n12    [MENTION, i, couldn, t, bear, to, watch, it, a...\n13    [MENTION, it, it, counts, idk, why, i, did, ei...\n14    [MENTION, i, would, ve, been, the, first, but,...\n15    [MENTION, i, wish, i, got, to, watch, it, with...\n16    [hollis, death, scene, will, hurt, me, severel...\n17                             [about, to, file, taxes]\n18    [MENTION, ahh, ive, always, wanted, to, see, r...\n19    [MENTION, oh, dear, were, you, drinking, out, ...\nName: text, dtype: object"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing 20 first twits\n",
    "dataset['text'][0:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:20:06.578886700Z",
     "start_time": "2024-06-01T14:20:06.572409300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...\n1    [22, 23, 6, 24, 25, 26, 27, 28, 29, 30, 31, 20...\n2    [3, 40, 41, 42, 43, 44, 45, 46, 47, 18, 48, 45...\n3    [53, 54, 55, 56, 57, 32, 58, 20, 59, 60, 0, 0,...\n4    [3, 61, 20, 7, 62, 63, 64, 65, 40, 66, 67, 68,...\n5    [3, 62, 45, 54, 75, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n6    [76, 8, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n7    [3, 78, 79, 43, 61, 72, 80, 81, 8, 82, 83, 8, ...\n8    [3, 88, 89, 90, 26, 91, 20, 0, 0, 0, 0, 0, 0, ...\n9    [3, 92, 93, 94, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nName: text, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_vocabulary(all_text):\n",
    "    vocabulary = {'PAD': 0, 'END': 1, 'UNK': 2}\n",
    "\n",
    "    for row in all_text:\n",
    "        for word in row:\n",
    "            if word not in vocabulary.keys():\n",
    "                vocabulary[word] = len(vocabulary.keys())\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def get_max_length(all_text: pd.DataFrame):\n",
    "    max_length = 0\n",
    "    for row in all_text:\n",
    "        if len(row) > max_length:\n",
    "            max_length = len(row)\n",
    "    return max_length\n",
    "\n",
    "def vectorization(all_text: pd.DataFrame, vocabulary: dict):\n",
    "    new_embedding = all_text.copy(deep=True)\n",
    "    for i in range(len(all_text)):\n",
    "        row = all_text.iloc[i]\n",
    "        new_embedding.iloc[i] = [vocabulary[token] for token in row]\n",
    "    return new_embedding\n",
    "\n",
    "def apply_padding(vectorized_dataset: pd.DataFrame, max_length: int, vocabulary: dict, pad_token: str):\n",
    "    copy_data = vectorized_dataset.copy(deep=True)\n",
    "    for i in range(len(vectorized_dataset)):\n",
    "        row = copy_data.iloc[i]\n",
    "        padding_token_id = vocabulary[pad_token]\n",
    "        copy_data.iloc[i] = row + [padding_token_id for _ in range(max_length - len(row))]\n",
    "    return copy_data\n",
    "\n",
    "vocabulary = create_vocabulary(dataset['text'])\n",
    "max_length = get_max_length(dataset['text'])\n",
    "vectorized_data = vectorization(dataset['text'], vocabulary)\n",
    "vectorized_padding = apply_padding(vectorized_data, max_length=max_length, vocabulary=vocabulary, pad_token='PAD')\n",
    "vectorized_padding[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T12:35:03.303165800Z",
     "start_time": "2024-06-01T12:33:02.368111700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
