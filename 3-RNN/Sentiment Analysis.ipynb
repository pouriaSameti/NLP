{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:06:58.971431500Z",
     "start_time": "2024-06-01T17:06:58.968430Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pouri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "import gensim.downloader as api\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:18:31.061946500Z",
     "start_time": "2024-06-01T14:18:30.125771Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      text  \\\n0        @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n1        is upset that he can't update his Facebook by ...   \n2        @Kenichan I dived many times for the ball. Man...   \n3          my whole body feels itchy and like its on fire    \n4        @nationwideclass no, it's not behaving at all....   \n...                                                    ...   \n1599995  Just woke up. Having no school is the best fee...   \n1599996  TheWDB.com - Very cool to hear old Walt interv...   \n1599997  Are you ready for your MoJo Makeover? Ask me f...   \n1599998  Happy 38th Birthday to my boo of alll time!!! ...   \n1599999  happy #charitytuesday @theNSPCC @SparksCharity...   \n\n                                 date             user  sentiment     query  \n0        Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n1        Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n2        Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n3        Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n4        Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n...                               ...              ...        ...       ...  \n1599995  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028          4  NO_QUERY  \n1599996  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards          4  NO_QUERY  \n1599997  Tue Jun 16 08:40:49 PDT 2009           bpbabe          4  NO_QUERY  \n1599998  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz          4  NO_QUERY  \n1599999  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris          4  NO_QUERY  \n\n[1600000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>date</th>\n      <th>user</th>\n      <th>sentiment</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>_TheSpecialOne_</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is upset that he can't update his Facebook by ...</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>scotthamilton</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>mattycus</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>ElleCTF</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>Karoli</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>Just woke up. Having no school is the best fee...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>AmandaMarie1028</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>TheWDBoards</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>bpbabe</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>tinydiamondz</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>RyanTrevMorris</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data/sentiment140.csv')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:18:36.376502Z",
     "start_time": "2024-06-01T14:18:32.240668700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "#remove web addresses, signs\n",
    "#change to lowercase\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_en_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub('@\\S+', '<MENTION>', text)\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "    # removing sign\n",
    "    text = ''.join([i for i in text if ord(i) not in [33, 34, 35, 36, 37, 38,\n",
    "                                                      39, 40, 41, 42, 43, 44,\n",
    "                                                      45, 46, 47, 58, 59, 60,\n",
    "                                                      61, 62, 63, 64, 91, 92,\n",
    "                                                      93, 94, 95, 96, 123, 124,\n",
    "                                                      125, 126, 1548, 1567]])\n",
    "    return text\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(clean_en_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:19:51.727994Z",
     "start_time": "2024-06-01T14:18:38.189954400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "dataset['text'] = dataset['text'].apply(lambda x: tokenizer.tokenize(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:20:06.564408900Z",
     "start_time": "2024-06-01T14:19:51.857091500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizer(text_list):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    words = [lemm.lemmatize(word) for word in text_list]\n",
    "    return words\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T12:33:02.302588200Z",
     "start_time": "2024-06-01T12:32:08.028043400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0     [MENTION, URL, awww, that, s, a, bummer, you, ...\n1     [is, upset, that, he, can, t, update, his, fac...\n2     [MENTION, i, dived, many, times, for, the, bal...\n3     [my, whole, body, feels, itchy, and, like, its...\n4     [MENTION, no, it, s, not, behaving, at, all, i...\n5                      [MENTION, not, the, whole, crew]\n6                                        [need, a, hug]\n7     [MENTION, hey, long, time, no, see, yes, rains...\n8              [MENTION, nope, they, didn, t, have, it]\n9                             [MENTION, que, me, muera]\n10     [spring, break, in, plain, city, it, s, snowing]\n11                     [i, just, re, pierced, my, ears]\n12    [MENTION, i, couldn, t, bear, to, watch, it, a...\n13    [MENTION, it, it, counts, idk, why, i, did, ei...\n14    [MENTION, i, would, ve, been, the, first, but,...\n15    [MENTION, i, wish, i, got, to, watch, it, with...\n16    [hollis, death, scene, will, hurt, me, severel...\n17                             [about, to, file, taxes]\n18    [MENTION, ahh, ive, always, wanted, to, see, r...\n19    [MENTION, oh, dear, were, you, drinking, out, ...\nName: text, dtype: object"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing 20 first twits\n",
    "dataset['text'][0:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:20:06.578886700Z",
     "start_time": "2024-06-01T14:20:06.572409300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...\n1    [22, 23, 6, 24, 25, 26, 27, 28, 29, 30, 31, 20...\n2    [3, 40, 41, 42, 43, 44, 45, 46, 47, 18, 48, 45...\n3    [53, 54, 55, 56, 57, 32, 58, 20, 59, 60, 0, 0,...\n4    [3, 61, 20, 7, 62, 63, 64, 65, 40, 66, 67, 68,...\n5    [3, 62, 45, 54, 75, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n6    [76, 8, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n7    [3, 78, 79, 43, 61, 72, 80, 81, 8, 82, 83, 8, ...\n8    [3, 88, 89, 90, 26, 91, 20, 0, 0, 0, 0, 0, 0, ...\n9    [3, 92, 93, 94, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\nName: text, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_vocabulary(all_text):\n",
    "    vocabulary = {'PAD': 0, 'END': 1, 'UNK': 2}\n",
    "\n",
    "    for row in all_text:\n",
    "        for word in row:\n",
    "            if word not in vocabulary.keys():\n",
    "                vocabulary[word] = len(vocabulary.keys())\n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def get_max_length(all_text: pd.DataFrame):\n",
    "    max_length = 0\n",
    "    for row in all_text:\n",
    "        if len(row) > max_length:\n",
    "            max_length = len(row)\n",
    "    return max_length\n",
    "\n",
    "def vectorization(all_text: pd.DataFrame, vocabulary: dict):\n",
    "    new_embedding = all_text.copy(deep=True)\n",
    "    for i in range(len(all_text)):\n",
    "        row = all_text.iloc[i]\n",
    "        new_embedding.iloc[i] = [vocabulary[token] for token in row]\n",
    "    return new_embedding\n",
    "\n",
    "def apply_padding(vectorized_dataset: pd.DataFrame, max_length: int, vocabulary: dict, pad_token: str):\n",
    "    copy_data = vectorized_dataset.copy(deep=True)\n",
    "    for i in range(len(vectorized_dataset)):\n",
    "        row = copy_data.iloc[i]\n",
    "        padding_token_id = vocabulary[pad_token]\n",
    "        copy_data.iloc[i] = row + [padding_token_id for _ in range(max_length - len(row))]\n",
    "    return copy_data\n",
    "\n",
    "vocabulary = create_vocabulary(dataset['text'])\n",
    "max_length = get_max_length(dataset['text'])\n",
    "vectorized_data = vectorization(dataset['text'], vocabulary)\n",
    "vectorized_padding = apply_padding(vectorized_data, max_length=max_length, vocabulary=vocabulary, pad_token='PAD')\n",
    "vectorized_padding[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T12:35:03.303165800Z",
     "start_time": "2024-06-01T12:33:02.368111700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      text  \\\n0        [MENTION, URL, awww, that, s, a, bummer, you, ...   \n1        [is, upset, that, he, can, t, update, his, fac...   \n2        [MENTION, i, dived, many, times, for, the, bal...   \n3        [my, whole, body, feels, itchy, and, like, its...   \n4        [MENTION, no, it, s, not, behaving, at, all, i...   \n...                                                    ...   \n1599995  [just, woke, up, having, no, school, is, the, ...   \n1599996  [thewdb, com, very, cool, to, hear, old, walt,...   \n1599997  [are, you, ready, for, your, mojo, makeover, a...   \n1599998  [happy, birthday, to, my, boo, of, alll, time,...   \n1599999  [happy, charitytuesday, MENTION, MENTION, MENT...   \n\n                                 date             user  sentiment     query  \\\n0        Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY   \n1        Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY   \n2        Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY   \n3        Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY   \n4        Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY   \n...                               ...              ...        ...       ...   \n1599995  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028          4  NO_QUERY   \n1599996  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards          4  NO_QUERY   \n1599997  Tue Jun 16 08:40:49 PDT 2009           bpbabe          4  NO_QUERY   \n1599998  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz          4  NO_QUERY   \n1599999  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris          4  NO_QUERY   \n\n                                                    tokens  \\\n0        [MENTION, URL, awww, that, s, a, bummer, you, ...   \n1        [is, upset, that, he, can, t, update, his, fac...   \n2        [MENTION, i, dived, many, times, for, the, bal...   \n3        [my, whole, body, feels, itchy, and, like, its...   \n4        [MENTION, no, it, s, not, behaving, at, all, i...   \n...                                                    ...   \n1599995  [just, woke, up, having, no, school, is, the, ...   \n1599996  [thewdb, com, very, cool, to, hear, old, walt,...   \n1599997  [are, you, ready, for, your, mojo, makeover, a...   \n1599998  [happy, birthday, to, my, boo, of, alll, time,...   \n1599999  [happy, charitytuesday, MENTION, MENTION, MENT...   \n\n                                                   encoded  \n0        [2, 40, 490, 17, 13, 5, 1174, 9, 3432, 52, 835...  \n1        [10, 776, 17, 88, 31, 15, 543, 187, 532, 127, ...  \n2        [2, 1, 51971, 314, 355, 12, 4, 1271, 1666, 3, ...  \n3            [6, 428, 801, 481, 2864, 8, 38, 72, 16, 1143]  \n4        [2, 41, 7, 13, 27, 10059, 26, 35, 1, 21, 594, ...  \n...                                                    ...  \n1599995  [23, 345, 33, 184, 41, 146, 10, 4, 176, 191, 223]  \n1599996  [267173, 587, 122, 201, 3, 282, 230, 12660, 39...  \n1599997  [39, 9, 209, 12, 47, 7606, 8092, 615, 18, 12, ...  \n1599998  [121, 269, 3, 6, 478, 14, 4973, 55, 11852, 715...  \n1599999                              [121, 17511, 2, 2, 2]  \n\n[1600000 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>date</th>\n      <th>user</th>\n      <th>sentiment</th>\n      <th>query</th>\n      <th>tokens</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[MENTION, URL, awww, that, s, a, bummer, you, ...</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>_TheSpecialOne_</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n      <td>[MENTION, URL, awww, that, s, a, bummer, you, ...</td>\n      <td>[2, 40, 490, 17, 13, 5, 1174, 9, 3432, 52, 835...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>scotthamilton</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n      <td>[is, upset, that, he, can, t, update, his, fac...</td>\n      <td>[10, 776, 17, 88, 31, 15, 543, 187, 532, 127, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[MENTION, i, dived, many, times, for, the, bal...</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>mattycus</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n      <td>[MENTION, i, dived, many, times, for, the, bal...</td>\n      <td>[2, 1, 51971, 314, 355, 12, 4, 1271, 1666, 3, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>ElleCTF</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n      <td>[6, 428, 801, 481, 2864, 8, 38, 72, 16, 1143]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[MENTION, no, it, s, not, behaving, at, all, i...</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>Karoli</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n      <td>[MENTION, no, it, s, not, behaving, at, all, i...</td>\n      <td>[2, 41, 7, 13, 27, 10059, 26, 35, 1, 21, 594, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1599995</th>\n      <td>[just, woke, up, having, no, school, is, the, ...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>AmandaMarie1028</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n      <td>[just, woke, up, having, no, school, is, the, ...</td>\n      <td>[23, 345, 33, 184, 41, 146, 10, 4, 176, 191, 223]</td>\n    </tr>\n    <tr>\n      <th>1599996</th>\n      <td>[thewdb, com, very, cool, to, hear, old, walt,...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>TheWDBoards</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n      <td>[thewdb, com, very, cool, to, hear, old, walt,...</td>\n      <td>[267173, 587, 122, 201, 3, 282, 230, 12660, 39...</td>\n    </tr>\n    <tr>\n      <th>1599997</th>\n      <td>[are, you, ready, for, your, mojo, makeover, a...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>bpbabe</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n      <td>[are, you, ready, for, your, mojo, makeover, a...</td>\n      <td>[39, 9, 209, 12, 47, 7606, 8092, 615, 18, 12, ...</td>\n    </tr>\n    <tr>\n      <th>1599998</th>\n      <td>[happy, birthday, to, my, boo, of, alll, time,...</td>\n      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n      <td>tinydiamondz</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n      <td>[happy, birthday, to, my, boo, of, alll, time,...</td>\n      <td>[121, 269, 3, 6, 478, 14, 4973, 55, 11852, 715...</td>\n    </tr>\n    <tr>\n      <th>1599999</th>\n      <td>[happy, charitytuesday, MENTION, MENTION, MENT...</td>\n      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n      <td>RyanTrevMorris</td>\n      <td>4</td>\n      <td>NO_QUERY</td>\n      <td>[happy, charitytuesday, MENTION, MENTION, MENT...</td>\n      <td>[121, 17511, 2, 2, 2]</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600000 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def encode(tokens, word2idx):\n",
    "    return [word2idx[token] for token in tokens if token in word2idx]\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "dataset['tokens'] = dataset['text']\n",
    "vocab = Counter()\n",
    "for tokens in dataset['tokens']:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# Create a mapping from word to index\n",
    "word2idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}  # Start indices from 1\n",
    "word2idx['<PAD>'] = 0  # Padding token\n",
    "\n",
    "dataset['encoded'] = dataset['tokens'].apply(lambda x: encode(x, word2idx))\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T16:59:57.274619500Z",
     "start_time": "2024-06-01T16:58:54.552151200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [2, 40, 490, 17, 13, 5, 1174, 9, 3432, 52, 835...\n1    [10, 776, 17, 88, 31, 15, 543, 187, 532, 127, ...\n2    [2, 1, 51971, 314, 355, 12, 4, 1271, 1666, 3, ...\n3    [6, 428, 801, 481, 2864, 8, 38, 72, 16, 1143, ...\n4    [2, 41, 7, 13, 27, 10059, 26, 35, 1, 21, 594, ...\n5    [2, 27, 4, 428, 2156, 0, 0, 0, 0, 0, 0, 0, 0, ...\n6    [94, 5, 913, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n7    [2, 153, 168, 55, 41, 69, 155, 2617, 5, 251, 1...\n8    [2, 786, 77, 145, 15, 20, 7, 0, 0, 0, 0, 0, 0,...\n9    [2, 2454, 18, 106981, 0, 0, 0, 0, 0, 0, 0, 0, ...\nName: padded, dtype: object"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label Encoding\n",
    "max_len = max(dataset['encoded'].apply(len))\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "dataset['padded'] = dataset['encoded'].apply(lambda x: pad_sequence(x, max_len))\n",
    "dataset['padded'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:05:01.859270600Z",
     "start_time": "2024-06-01T17:04:31.686520800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#label Encoding\n",
    "label2idx = {label: idx for idx, label in enumerate(dataset['sentiment'].unique())}\n",
    "dataset['label_idx'] = dataset['sentiment'].map(label2idx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(train_df['padded'].tolist(), train_df['label_idx'].tolist())\n",
    "val_dataset = TextDataset(val_df['padded'].tolist(), val_df['label_idx'].tolist())\n",
    "test_dataset = TextDataset(test_df['padded'].tolist(), test_df['label_idx'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:07:04.827151400Z",
     "start_time": "2024-06-01T17:07:03.959264800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[   2,  163,    9,  ...,    0,    0,    0],\n         [   2, 3190,  662,  ...,    0,    0,    0],\n         [  19,   72,   64,  ...,    0,    0,    0],\n         ...,\n         [   2,   32,    7,  ...,    0,    0,    0],\n         [   1,   23, 3268,  ...,    0,    0,    0],\n         [   2,    9,  319,  ...,    0,    0,    0]]),\n tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n         1, 1, 0, 0, 0, 1, 1, 1])]"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_loader).__next__()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:08:10.617035900Z",
     "start_time": "2024-06-01T17:08:10.521153400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2idx['<PAD>'])\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embed_size = 100\n",
    "hidden_size = 128\n",
    "output_size = len(label2idx)\n",
    "num_layers = 2\n",
    "\n",
    "model = TextRNN(vocab_size, embed_size, hidden_size, output_size, num_layers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:09:18.686686300Z",
     "start_time": "2024-06-01T17:09:18.545958Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[176], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(output, labels)\n\u001B[0;32m     12\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 13\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[0;32m     16\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    386\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    387\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    388\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    389\u001B[0m             )\n\u001B[1;32m--> 391\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    392\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    394\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:143\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;129m@_use_grad_for_differentiable\u001B[39m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, closure\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    137\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Perform a single optimization step.\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001B[39;00m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;124;03m            and returns the loss.\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 143\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cuda_graph_capture_health_check()\n\u001B[0;32m    145\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    146\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:339\u001B[0m, in \u001B[0;36mOptimizer._cuda_graph_capture_health_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_cuda_graph_capture_health_check\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    328\u001B[0m     \u001B[38;5;66;03m# Note [torch.compile x capturable]\u001B[39;00m\n\u001B[0;32m    329\u001B[0m     \u001B[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    336\u001B[0m     \u001B[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001B[39;00m\n\u001B[0;32m    337\u001B[0m     \u001B[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001B[39;00m\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_compiling() \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_built() \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[1;32m--> 339\u001B[0m         capturing \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_current_stream_capturing()\n\u001B[0;32m    341\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m capturing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups):\n\u001B[0;32m    342\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    343\u001B[0m                                \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m    344\u001B[0m                                \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but param_groups\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m capturable is False.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\cuda\\graphs.py:28\u001B[0m, in \u001B[0;36mis_current_stream_capturing\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_current_stream_capturing\u001B[39m():\n\u001B[0;32m     24\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \n\u001B[0;32m     26\u001B[0m \u001B[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _cuda_isCurrentStreamCapturing()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            output = model(texts)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T17:09:20.716617700Z",
     "start_time": "2024-06-01T17:09:20.085248500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
