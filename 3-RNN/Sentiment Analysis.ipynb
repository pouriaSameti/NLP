{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:47:21.827128900Z",
     "start_time": "2024-06-06T18:47:16.644082800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pouri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "import gensim.downloader as api\n",
    "# w2v = api.load('word2vec-google-news-300')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:47:26.528847800Z",
     "start_time": "2024-06-06T18:47:21.844139900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  \\\n0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n1  is upset that he can't update his Facebook by ...   \n2  @Kenichan I dived many times for the ball. Man...   \n3    my whole body feels itchy and like its on fire    \n4  @nationwideclass no, it's not behaving at all....   \n5                      @Kwesidei not the whole crew    \n6                                        Need a hug    \n7  @LOLTrish hey  long time no see! Yes.. Rains a...   \n8               @Tatiana_K nope they didn't have it    \n9                          @twittera que me muera ?    \n\n                           date             user  sentiment     query  \n0  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_          0  NO_QUERY  \n1  Mon Apr 06 22:19:49 PDT 2009    scotthamilton          0  NO_QUERY  \n2  Mon Apr 06 22:19:53 PDT 2009         mattycus          0  NO_QUERY  \n3  Mon Apr 06 22:19:57 PDT 2009          ElleCTF          0  NO_QUERY  \n4  Mon Apr 06 22:19:57 PDT 2009           Karoli          0  NO_QUERY  \n5  Mon Apr 06 22:20:00 PDT 2009         joy_wolf          0  NO_QUERY  \n6  Mon Apr 06 22:20:03 PDT 2009          mybirch          0  NO_QUERY  \n7  Mon Apr 06 22:20:03 PDT 2009             coZZ          0  NO_QUERY  \n8  Mon Apr 06 22:20:05 PDT 2009  2Hood4Hollywood          0  NO_QUERY  \n9  Mon Apr 06 22:20:09 PDT 2009          mimismo          0  NO_QUERY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>date</th>\n      <th>user</th>\n      <th>sentiment</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>_TheSpecialOne_</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is upset that he can't update his Facebook by ...</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>scotthamilton</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>mattycus</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>my whole body feels itchy and like its on fire</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>ElleCTF</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>Karoli</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>@Kwesidei not the whole crew</td>\n      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n      <td>joy_wolf</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Need a hug</td>\n      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n      <td>mybirch</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n      <td>coZZ</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>@Tatiana_K nope they didn't have it</td>\n      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n      <td>2Hood4Hollywood</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>@twittera que me muera ?</td>\n      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n      <td>mimismo</td>\n      <td>0</td>\n      <td>NO_QUERY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data/sentiment140.csv')\n",
    "dataset[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:47:30.581661700Z",
     "start_time": "2024-06-06T18:47:26.525373Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0     MENTION   URL    awww  that s a bummer   you ...\n1    is upset that he can t update his facebook by ...\n2     MENTION  i dived many times for the ball  man...\n3      my whole body feels itchy and like its on fire \n4     MENTION  no  it s not behaving at all  i m ma...\n5                         MENTION  not the whole crew \n6                                          need a hug \n7     MENTION  hey  long time no see  yes   rains a...\n8                   MENTION  nope they didn t have it \n9                             MENTION  que me muera   \nName: text, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove web addresses, signs\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_en_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '<URL>', text)\n",
    "    text = re.sub('@\\S+', '<MENTION>', text)\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "    # removing sign\n",
    "    text = ''.join([i for i in text if ord(i) not in [33, 34, 35, 36, 37, 38,\n",
    "                                                      39, 40, 41, 42, 43, 44,\n",
    "                                                      45, 46, 47, 58, 59, 60,\n",
    "                                                      61, 62, 63, 64, 91, 92,\n",
    "                                                      93, 94, 95, 96, 123, 124,\n",
    "                                                      125, 126, 1548, 1567]])\n",
    "    return text\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(clean_en_text)\n",
    "dataset['text'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:48:34.931301900Z",
     "start_time": "2024-06-06T18:47:30.704146800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [MENTION, URL, awww, that, s, a, bummer, you, ...\n1    [is, upset, that, he, can, t, update, his, fac...\n2    [MENTION, i, dived, many, times, for, the, bal...\n3    [my, whole, body, feels, itchy, and, like, its...\n4    [MENTION, no, it, s, not, behaving, at, all, i...\n5                     [MENTION, not, the, whole, crew]\n6                                       [need, a, hug]\n7    [MENTION, hey, long, time, no, see, yes, rains...\n8             [MENTION, nope, they, didn, t, have, it]\n9                            [MENTION, que, me, muera]\nName: text, dtype: object"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "dataset['text'] = dataset['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "dataset['text'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:48:48.577156Z",
     "start_time": "2024-06-06T18:48:34.934437600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatizer(text_list):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    words = [lemm.lemmatize(word) for word in text_list]\n",
    "    return words\n",
    "\n",
    "\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:50:12.971691100Z",
     "start_time": "2024-06-06T18:48:48.580232400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0     [MENTION, URL, awww, that, s, a, bummer, you, ...\n1     [is, upset, that, he, can, t, update, his, fac...\n2     [MENTION, i, dived, many, time, for, the, ball...\n3     [my, whole, body, feel, itchy, and, like, it, ...\n4     [MENTION, no, it, s, not, behaving, at, all, i...\n5                      [MENTION, not, the, whole, crew]\n6                                        [need, a, hug]\n7     [MENTION, hey, long, time, no, see, yes, rain,...\n8              [MENTION, nope, they, didn, t, have, it]\n9                             [MENTION, que, me, muera]\n10     [spring, break, in, plain, city, it, s, snowing]\n11                      [i, just, re, pierced, my, ear]\n12    [MENTION, i, couldn, t, bear, to, watch, it, a...\n13    [MENTION, it, it, count, idk, why, i, did, eit...\n14    [MENTION, i, would, ve, been, the, first, but,...\n15    [MENTION, i, wish, i, got, to, watch, it, with...\n16    [hollis, death, scene, will, hurt, me, severel...\n17                               [about, to, file, tax]\n18    [MENTION, ahh, ive, always, wanted, to, see, r...\n19    [MENTION, oh, dear, were, you, drinking, out, ...\nName: text, dtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Showing 20 first twits\n",
    "dataset['text'][0:20]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:50:12.990812700Z",
     "start_time": "2024-06-06T18:50:12.984181600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [2, 41, 499, 17, 13, 5, 1149, 9, 3306, 54, 822...\n1    [10, 757, 17, 85, 32, 15, 391, 195, 541, 130, ...\n2    [2, 1, 47210, 320, 50, 12, 4, 944, 1641, 3, 85...\n3          [7, 434, 771, 95, 2791, 8, 38, 6, 16, 1093]\n4    [2, 42, 6, 13, 27, 9230, 26, 36, 1, 21, 597, 1...\n5                                [2, 27, 4, 434, 2098]\n6                                         [81, 5, 503]\n7    [2, 158, 176, 50, 42, 71, 153, 252, 5, 249, 11...\n8                         [2, 780, 78, 151, 15, 20, 6]\n9                                 [2, 2400, 18, 99172]\nName: encoded, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def encode(tokens, word2idx):\n",
    "    return [word2idx[token] for token in tokens if token in word2idx]\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "dataset['tokens'] = dataset['text']\n",
    "vocab = Counter()\n",
    "for tokens in dataset['tokens']:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "\n",
    "word2idx = {word: idx + 1 for idx, (word, _) in enumerate(vocab.most_common())}\n",
    "word2idx['<PAD>'] = 0  # Padding token\n",
    "\n",
    "dataset['encoded'] = dataset['tokens'].apply(lambda x: encode(x, word2idx))\n",
    "dataset['encoded'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:50:25.380635900Z",
     "start_time": "2024-06-06T18:50:13.133123900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "0    [2, 41, 499, 17, 13, 5, 1149, 9, 3306, 54, 822...\n1    [10, 757, 17, 85, 32, 15, 391, 195, 541, 130, ...\n2    [2, 1, 47210, 320, 50, 12, 4, 944, 1641, 3, 85...\n3    [7, 434, 771, 95, 2791, 8, 38, 6, 16, 1093, 0,...\n4    [2, 42, 6, 13, 27, 9230, 26, 36, 1, 21, 597, 1...\n5    [2, 27, 4, 434, 2098, 0, 0, 0, 0, 0, 0, 0, 0, ...\n6    [81, 5, 503, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n7    [2, 158, 176, 50, 42, 71, 153, 252, 5, 249, 11...\n8    [2, 780, 78, 151, 15, 20, 6, 0, 0, 0, 0, 0, 0,...\n9    [2, 2400, 18, 99172, 0, 0, 0, 0, 0, 0, 0, 0, 0...\nName: padded, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label Encoding\n",
    "max_len = max(dataset['encoded'].apply(len))\n",
    "\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [word2idx['<PAD>']] * (max_len - len(seq))\n",
    "\n",
    "dataset['padded'] = dataset['encoded'].apply(lambda x: pad_sequence(x, max_len))\n",
    "dataset['padded'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:50:33.357342200Z",
     "start_time": "2024-06-06T18:50:25.381635600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#label Encoding\n",
    "label2idx = {label: idx for idx, label in enumerate(dataset['sentiment'].unique())}\n",
    "dataset['label_idx'] = dataset['sentiment'].map(label2idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:50:33.441577Z",
     "start_time": "2024-06-06T18:50:33.364341800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(train_df['padded'].tolist(), train_df['label_idx'].tolist())\n",
    "val_dataset = TextDataset(val_df['padded'].tolist(), val_df['label_idx'].tolist())\n",
    "test_dataset = TextDataset(test_df['padded'].tolist(), test_df['label_idx'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T19:45:22.099674800Z",
     "start_time": "2024-06-06T19:45:17.707659Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:51:24.755305400Z",
     "start_time": "2024-06-06T18:50:34.641284200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "embedding_matrix = np.random.normal(size=(len(word2idx), embedding_dim))\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[idx] = word2vec_model[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "embedding_matrix[word2idx['<PAD>']] = np.zeros((embedding_dim,))  # Padding token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:51:36.282600Z",
     "start_time": "2024-06-06T18:51:24.763080Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#\n",
    "# class TextRNN(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, embedding_matrix=None):\n",
    "#         super(TextRNN, self).__init__()\n",
    "#         if embedding_matrix is not None:\n",
    "#             self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n",
    "#         else:\n",
    "#             self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2idx['<PAD>'])\n",
    "#         self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.embedding(x)\n",
    "#         h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(x.device)\n",
    "#         out, _ = self.rnn(x, h0)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:51:36.330148900Z",
     "start_time": "2024-06-06T18:51:36.318315600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# vocab_size = len(word2idx)\n",
    "# embed_size = embedding_dim  # 300\n",
    "# hidden_size = 128\n",
    "# output_size = len(label2idx)\n",
    "# num_layers = 2\n",
    "#\n",
    "# model = TextRNN(vocab_size, embed_size, hidden_size, output_size, num_layers, embedding_matrix=embedding_matrix).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# num_epochs = 5\n",
    "#\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for texts, labels in train_loader:\n",
    "#         texts, labels = texts.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(texts)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for texts, labels in val_loader:\n",
    "#             texts, labels = texts.to(device), labels.to(device)\n",
    "#             output = model(texts)\n",
    "#             loss = criterion(output, labels)\n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = torch.max(output, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "#\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:51:36.339213800Z",
     "start_time": "2024-06-06T18:51:36.335200300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# for x_test, y_test in test_loader:\n",
    "#     val_loss = 0\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "#\n",
    "#     x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "#     output = model(x_test)\n",
    "#     loss = criterion(output, y_test)\n",
    "#     val_loss += loss.item()\n",
    "#     _, predicted = torch.max(output, 1)\n",
    "#     total += y_test.size(0)\n",
    "#     correct += (predicted == y_test).sum().item()\n",
    "#\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f'Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:51:36.353699200Z",
     "start_time": "2024-06-06T18:51:36.350466300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GRUText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, embedding_matrix=None):\n",
    "        super(GRUText, self).__init__()\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2idx['<PAD>'])\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.gru.num_layers, x.size(0), self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T18:51:37.906992500Z",
     "start_time": "2024-06-06T18:51:37.874690200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6940355896949768, Val Loss: 0.6932887777169545, Val Accuracy: 49.7725%\n",
      "Epoch 2/10, Loss: 0.6937782168388367, Val Loss: 0.6932340858618419, Val Accuracy: 49.7725%\n",
      "Epoch 3/10, Loss: 0.6926338076591492, Val Loss: 0.6931459022204082, Val Accuracy: 50.2275%\n",
      "Epoch 4/10, Loss: 0.6933507323265076, Val Loss: 0.693166630188624, Val Accuracy: 49.7725%\n",
      "Epoch 5/10, Loss: 0.6933175921440125, Val Loss: 0.6931629425048829, Val Accuracy: 49.7725%\n",
      "Epoch 6/10, Loss: 0.6942907571792603, Val Loss: 0.6933521067460378, Val Accuracy: 49.7725%\n",
      "Epoch 7/10, Loss: 0.6927319169044495, Val Loss: 0.693139804538091, Val Accuracy: 50.2275%\n",
      "Epoch 8/10, Loss: 0.7000269293785095, Val Loss: 0.693338436794281, Val Accuracy: 50.35541666666666%\n",
      "Epoch 9/10, Loss: 0.6986992955207825, Val Loss: 0.6932215099175771, Val Accuracy: 51.20291666666667%\n",
      "Epoch 10/10, Loss: 0.4679205119609833, Val Loss: 0.46014162616729737, Val Accuracy: 78.39916666666667%\n"
     ]
    }
   ],
   "source": [
    "# Training GRU based model\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embed_size = embedding_dim  # 300\n",
    "hidden_size = 128\n",
    "output_size = len(label2idx)\n",
    "num_layers = 2\n",
    "\n",
    "model = GRUText(vocab_size, embed_size, hidden_size, output_size, num_layers, embedding_matrix=embedding_matrix).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(texts)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            output = model(texts)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T19:30:43.756843Z",
     "start_time": "2024-06-06T19:11:50.417396200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3631858229637146, Val Loss: 9.68495527903239e-05, Val Accuracy: 84.375%\n"
     ]
    }
   ],
   "source": [
    "for x_test, y_test in test_loader:\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "    output = model(x_test)\n",
    "    loss = criterion(output, y_test)\n",
    "    val_loss += loss.item()\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y_test.size(0)\n",
    "    correct += (predicted == y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T19:31:59.160010300Z",
     "start_time": "2024-06-06T19:31:59.130948500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class LSTMText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=1, embedding_matrix=None):\n",
    "        super(LSTMText, self).__init__()\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=word2idx['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T19:49:21.794400200Z",
     "start_time": "2024-06-06T19:49:21.789778700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6941134929656982, Val Loss: 0.6933073308944702, Val Accuracy: 49.7725%\n",
      "Epoch 2/20, Loss: 0.6946383714675903, Val Loss: 0.6934506315390269, Val Accuracy: 49.7725%\n",
      "Epoch 3/20, Loss: 0.6928666830062866, Val Loss: 0.6931367210388184, Val Accuracy: 50.2275%\n",
      "Epoch 4/20, Loss: 0.692689061164856, Val Loss: 0.6931423070271809, Val Accuracy: 50.2275%\n",
      "Epoch 5/20, Loss: 0.6931376457214355, Val Loss: 0.6931465291023254, Val Accuracy: 50.2275%\n",
      "Epoch 6/20, Loss: 0.6936068534851074, Val Loss: 0.6932030660947164, Val Accuracy: 49.7725%\n",
      "Epoch 7/20, Loss: 0.6933059096336365, Val Loss: 0.6931615688959758, Val Accuracy: 49.7725%\n",
      "Epoch 8/20, Loss: 0.6926425695419312, Val Loss: 0.6931453162988027, Val Accuracy: 50.2275%\n",
      "Epoch 9/20, Loss: 0.6936168074607849, Val Loss: 0.6932048579851786, Val Accuracy: 49.7725%\n",
      "Epoch 10/20, Loss: 0.6934311389923096, Val Loss: 0.6931769119898478, Val Accuracy: 49.7725%\n",
      "Epoch 11/20, Loss: 0.693007230758667, Val Loss: 0.6931394038041433, Val Accuracy: 50.2275%\n",
      "Epoch 12/20, Loss: 0.6940523386001587, Val Loss: 0.6932927575588226, Val Accuracy: 49.7725%\n",
      "Epoch 13/20, Loss: 0.6933619379997253, Val Loss: 0.6931680096944173, Val Accuracy: 49.7725%\n",
      "Epoch 14/20, Loss: 0.6937001347541809, Val Loss: 0.6932194298585256, Val Accuracy: 49.7725%\n",
      "Epoch 15/20, Loss: 0.6931607127189636, Val Loss: 0.6931484437942504, Val Accuracy: 49.7725%\n",
      "Epoch 16/20, Loss: 0.6928675174713135, Val Loss: 0.6931368091265361, Val Accuracy: 50.2275%\n",
      "Epoch 17/20, Loss: 0.6928145885467529, Val Loss: 0.6931374346733093, Val Accuracy: 50.2275%\n",
      "Epoch 18/20, Loss: 0.6923609375953674, Val Loss: 0.6931834640026092, Val Accuracy: 50.2275%\n",
      "Epoch 19/20, Loss: 0.6935563087463379, Val Loss: 0.6931950550238292, Val Accuracy: 49.7725%\n",
      "Epoch 20/20, Loss: 0.6931028366088867, Val Loss: 0.6931442143599192, Val Accuracy: 50.2275%\n"
     ]
    }
   ],
   "source": [
    "# Training LSTM based model\n",
    "\n",
    "vocab_size = len(word2idx)\n",
    "embed_size = embedding_dim  # 300\n",
    "hidden_size = 128\n",
    "output_size = len(label2idx)\n",
    "num_layers = 2\n",
    "\n",
    "lstm_model = LSTMText(vocab_size, embed_size, hidden_size, output_size, num_layers, embedding_matrix=embedding_matrix).to(device)\n",
    "lstm_criterion = nn.CrossEntropyLoss()\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        lstm_optimizer.zero_grad()\n",
    "        output = lstm_model(x_train)\n",
    "        loss = lstm_criterion(output, y_train)\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            output = lstm_model(x_val)\n",
    "            loss = lstm_criterion(output, y_val)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T21:27:06.788715800Z",
     "start_time": "2024-06-06T20:20:36.313506600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6931474208831787, Val Loss: 0.0001848393122355143, Val Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "for x_test, y_test in test_loader:\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "    output = lstm_model(x_test)\n",
    "    loss = lstm_criterion(output, y_test)\n",
    "    val_loss += loss.item()\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    total += y_test.size(0)\n",
    "    correct += (predicted == y_test).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Loss: {loss.item()}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {accuracy}%')\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T21:28:25.433773400Z",
     "start_time": "2024-06-06T21:28:25.398643200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
