{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:55:52.581706500Z",
     "start_time": "2024-04-27T13:55:47.794139800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pouri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download requirements\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:55:54.097335800Z",
     "start_time": "2024-04-27T13:55:52.582749100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               comment\n0    نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...\n1    چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...\n2                                      پراید ستون جدید\n3    اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...\n4    گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا...\n..                                                 ...\n235                   پوشش دهی صفر.اصلا پیشنهاد نمیکنم\n236  نصب این فن خیلی راحته و دردسر  زیادی نداره درض...\n237                                           بی کیفیت\n238  سلام ٬ چندماهی میشه این پاور بانک رو تهیه کردم...\n239  این مدل رو من چند روزی هست که دریافت کردم.توی ...\n\n[240 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ق...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>چند ماهی میشه که گرفتمش‌. برای برنامه نویسی و ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>پراید ستون جدید</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ا...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>پوشش دهی صفر.اصلا پیشنهاد نمیکنم</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>نصب این فن خیلی راحته و دردسر  زیادی نداره درض...</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>بی کیفیت</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>سلام ٬ چندماهی میشه این پاور بانک رو تهیه کردم...</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>این مدل رو من چند روزی هست که دریافت کردم.توی ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>240 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/digikala_comment.csv')\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:55:54.193979400Z",
     "start_time": "2024-04-27T13:55:54.096085300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               comment\n0    [نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ...\n1    [چند ماهی میشه که گرفتمش‌., برای برنامه نویسی ...\n2                                    [پراید ستون جدید]\n3    [اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره...\n4    [گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ...\n..                                                 ...\n235                 [پوشش دهی صفر.اصلا پیشنهاد نمیکنم]\n236  [نصب این فن خیلی راحته و دردسر  زیادی نداره در...\n237                                         [بی کیفیت]\n238  [سلام ٬ چندماهی میشه این پاور بانک رو تهیه کرد...\n239  [این مدل رو من چند روزی هست که دریافت کردم.توی...\n\n[240 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[نسبت به قیمتش ارزش خرید داره\\nجاداره، طراحیش ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[چند ماهی میشه که گرفتمش‌., برای برنامه نویسی ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[پراید ستون جدید]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[گوسی هو اوی p10 lite سیپیو و دوربین و رمش از ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>[پوشش دهی صفر.اصلا پیشنهاد نمیکنم]</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>[نصب این فن خیلی راحته و دردسر  زیادی نداره در...</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>[بی کیفیت]</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>[سلام ٬ چندماهی میشه این پاور بانک رو تهیه کرد...</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>[این مدل رو من چند روزی هست که دریافت کردم.توی...</td>\n    </tr>\n  </tbody>\n</table>\n<p>240 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Tokenizing\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_tokenizing(text):\n",
    "    return sentence_tokenizing(text)\n",
    "\n",
    "data['comment'] = data.apply(lambda text:sent_tokenize(text['comment']), axis=1)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:55:54.195976100Z",
     "start_time": "2024-04-27T13:55:54.130223400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               comment\n0    [نسبت به قیمتش ارزش خرید دارهجاداره طراحیش قشن...\n1    [چند ماهی میشه که گرفتمش‌, برای برنامه نویسی و...\n2                                    [پراید ستون جدید]\n3    [اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره...\n4    [گوسی هو اوی   سیپیو و دوربین و رمش از این خیل...\n..                                                 ...\n235                  [پوشش دهی صفراصلا پیشنهاد نمیکنم]\n236  [نصب این فن خیلی راحته و دردسر  زیادی نداره در...\n237                                         [بی کیفیت]\n238  [سلام ٬ چندماهی میشه این پاور بانک رو تهیه کرد...\n239  [این مدل رو من چند روزی هست که دریافت کردمتوی ...\n\n[240 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[نسبت به قیمتش ارزش خرید دارهجاداره طراحیش قشن...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[چند ماهی میشه که گرفتمش‌, برای برنامه نویسی و...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[پراید ستون جدید]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[اقا همه چیش خوبه فقط از پایین زیاد حاشیه داره...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[گوسی هو اوی   سیپیو و دوربین و رمش از این خیل...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>235</th>\n      <td>[پوشش دهی صفراصلا پیشنهاد نمیکنم]</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>[نصب این فن خیلی راحته و دردسر  زیادی نداره در...</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>[بی کیفیت]</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>[سلام ٬ چندماهی میشه این پاور بانک رو تهیه کرد...</td>\n    </tr>\n    <tr>\n      <th>239</th>\n      <td>[این مدل رو من چند روزی هست که دریافت کردمتوی ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>240 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_fa_text(text):\n",
    "    #removing english characters and signs\n",
    "    text = ''.join([i for i in text if not ((65 <= ord(i) <91)\n",
    "                                            or (97 <= ord(i) < 123)\n",
    "                                            or (48 <= ord(i) < 58))])\n",
    "\n",
    "    #removing nbsp\n",
    "    text_list = []\n",
    "    for char in text:\n",
    "        if ord(char) == 160:\n",
    "            text_list.append(' ')\n",
    "            continue\n",
    "        text_list.append(char)\n",
    "    text = ''.join(text_list)\n",
    "\n",
    "    # removing sign\n",
    "    text = ''.join([i for i in text if ord(i) not in [33, 34, 35, 36, 37, 38,\n",
    "                                                      39, 40, 41, 42, 43, 44,\n",
    "                                                      45, 46, 47, 58, 59, 60,\n",
    "                                                      61, 62, 63, 64, 91, 92,\n",
    "                                                      93, 94, 95, 96, 123, 124,\n",
    "                                                      125, 126, 1548, 1567]])\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def apply_clean(list_sentence: list):\n",
    "    new_list = []\n",
    "    for sent in list_sentence:\n",
    "        new_list.append(clean_fa_text(sent))\n",
    "    return new_list\n",
    "\n",
    "data['comment'] = data['comment'].apply(apply_clean)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T13:55:54.494714Z",
     "start_time": "2024-04-27T13:55:54.186468700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#text normalizing\n",
    "from hazm import Normalizer\n",
    "\n",
    "def apply_normalizer(list_sentence: list):\n",
    "    new_list = []\n",
    "    norm = Normalizer()\n",
    "    for sent in list_sentence:\n",
    "        new_list.append(norm.normalize(sent))\n",
    "    return new_list\n",
    "\n",
    "data['comment'] = data['comment'].apply(apply_normalizer)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-27T13:55:54.279024800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#concat all sentences\n",
    "def append_all(data, column: str):\n",
    "    all_sentences = []\n",
    "    for sent in data[column]:\n",
    "        all_sentences.extend(sent)\n",
    "    return all_sentences\n",
    "\n",
    "all_sentences = append_all(data, 'comment')\n",
    "all_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#splite sentence wrt words\n",
    "\n",
    "def split_sentences(data: list):\n",
    "    dat=[]\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    return dat\n",
    "\n",
    "all_words = split_sentences(all_sentences)\n",
    "all_words"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#unigram model\n",
    "\n",
    "def create_unigram(data):\n",
    "    list_of_unigrams = []\n",
    "    unigram_counts = {}\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        list_of_unigrams.append(data[i])\n",
    "\n",
    "        if data[i] not in unigram_counts:\n",
    "            unigram_counts[data[i]] = 0\n",
    "        unigram_counts[data[i]] += 1\n",
    "\n",
    "\n",
    "    return list_of_unigrams, unigram_counts\n",
    "\n",
    "def unigram_probability(list_of_unigram, unigram_counts: dict, k: int=1):\n",
    "    prob_list = {}\n",
    "    len_all_words = sum(unigram_counts.values())\n",
    "\n",
    "    for unigram in list_of_unigram:\n",
    "        prob_list[unigram] = (unigram_counts.get(unigram) + k)/(len_all_words + k*len_all_words)\n",
    "    return prob_list\n",
    "\n",
    "list_of_unigram, first_unigram_counts = create_unigram(all_words)\n",
    "unigram_prob = unigram_probability(list_of_unigram, first_unigram_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bigram model\n",
    "\n",
    "def create_bigram(data):\n",
    "    list_of_bigrams = []\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "\n",
    "    for i in range(len(data)-1):\n",
    "        if i < len(data) - 1 and data[i+1]:\n",
    "            list_of_bigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "            if (data[i], data[i+1]) in bigram_counts:\n",
    "                bigram_counts[(data[i], data[i + 1])] += 1\n",
    "            else:\n",
    "                bigram_counts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "        if data[i] in unigram_counts:\n",
    "            unigram_counts[data[i]] += 1\n",
    "        else:\n",
    "            unigram_counts[data[i]] = 1\n",
    "\n",
    "    return list_of_bigrams, unigram_counts, bigram_counts\n",
    "\n",
    "\n",
    "def bigram_probability(list_of_bigrams, unigram_counts, bigram_counts, k: int=1):\n",
    "    prob_list = {}\n",
    "    for bigram in list_of_bigrams:\n",
    "        word1 = bigram[0]\n",
    "        prob_list[bigram] = (bigram_counts.get(bigram))/(unigram_counts.get(word1) + len(unigram_counts)*k)\n",
    "    return prob_list\n",
    "\n",
    "\n",
    "def get_bigram(sentence: str):\n",
    "    splt = sentence.split()\n",
    "    bilist = []\n",
    "\n",
    "    for i in range(len(splt) - 1):\n",
    "        bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "    return bilist\n",
    "\n",
    "\n",
    "list_of_bigrams, unigram_counts, bigram_counts = create_bigram(all_words)\n",
    "bigram_prob = bigram_probability(list_of_bigrams, unigram_counts, bigram_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#trigram model\n",
    "\n",
    "def create_trigram(data):\n",
    "    list_of_trigrams = []\n",
    "    trigram_counts = {}\n",
    "    bigram_counts = {}\n",
    "\n",
    "\n",
    "    for i in range(len(data)-2):\n",
    "        if i < len(data) - 2 and data[i+2]:\n",
    "            list_of_trigrams.append((data[i], data[i + 1], data[i + 2]))\n",
    "\n",
    "            if (data[i], data[i + 1], data[i + 2]) in trigram_counts:\n",
    "                trigram_counts[(data[i], data[i + 1], data[i + 2])] += 1\n",
    "            else:\n",
    "                trigram_counts[(data[i], data[i + 1], data[i + 2])] = 1\n",
    "\n",
    "        if (data[i], data[i+1]) in bigram_counts:\n",
    "            bigram_counts[(data[i], data[i + 1])] += 1\n",
    "        else:\n",
    "            bigram_counts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "    return list_of_trigrams, bigram_counts, trigram_counts\n",
    "\n",
    "\n",
    "def trigram_probability(list_of_trigrams, bigram_counts, trigram_counts, k: int=1):\n",
    "    prob_list = {}\n",
    "    for trigram in list_of_trigrams:\n",
    "        word1 = trigram[0]\n",
    "        word2 = trigram[1]\n",
    "        prob_list[trigram] = (trigram_counts.get(trigram))/(bigram_counts.get((word1, word2)) + len(bigram_counts)*k)\n",
    "    return prob_list\n",
    "\n",
    "def get_trigram(sentence: str):\n",
    "    splt = sentence.split()\n",
    "    bilist = []\n",
    "\n",
    "    for i in range(len(splt) - 2):\n",
    "        bilist.append((splt[i], splt[i + 1], splt[i + 2]))\n",
    "\n",
    "    return bilist\n",
    "\n",
    "list_of_trigrams, bigram_counts, trigram_counts = create_trigram(all_words)\n",
    "trigram_prob = trigram_probability(list_of_trigrams, bigram_counts, trigram_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#most probable unigram\n",
    "most_prob_unigram = pd.DataFrame(unigram_prob.items(), columns=['unigram', 'probability']).sort_values(by='probability', ascending=False)[:8]\n",
    "most_prob_unigram"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#most probable bigrams\n",
    "most_prob_bigram = pd.DataFrame(bigram_prob.items(), columns=['bigram', 'probability']).sort_values(by='probability', ascending=False)[:8]\n",
    "most_prob_bigram"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#most probable bigrams\n",
    "most_prob_trigram = pd.DataFrame(trigram_prob.items(), columns=['trigram', 'probability']).sort_values(by='probability', ascending=False)[:8]\n",
    "most_prob_trigram"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#probability calculation of test set\n",
    "\n",
    "from hazm import Normalizer\n",
    "\n",
    "def get_probability(list_sentence: list, ngram_probability):\n",
    "    probability = 1\n",
    "    for i in range(len(list_sentence)):\n",
    "        if list_sentence[i] in ngram_probability:\n",
    "            probability *= ngram_probability[list_sentence[i]]\n",
    "        else:\n",
    "            probability *= 0.00001\n",
    "    return probability\n",
    "\n",
    "def get_perplexity(list_sentence: list, ngram_probability):\n",
    "    probability = 1\n",
    "    for i in range(len(list_sentence)):\n",
    "        if list_sentence[i] in ngram_probability:\n",
    "            probability *= ngram_probability[list_sentence[i]]\n",
    "        else:\n",
    "            probability *= 0.00001\n",
    "\n",
    "    m = len(list_sentence)\n",
    "    perplexity = np.power(1/probability, 1/m)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "sentence1 = 'این لپتاپ سخت افزار خیلی قوی داره و از پس هرکاری به راحتی برمیاد'\n",
    "sentence2 = 'این ساعت بسیار زیبا طراحی و ساخته شده'\n",
    "sentence3 = 'یک محصول باکیفیت ایرانی که حقیقتا جای حمایت داره'\n",
    "sentence4 = 'بوش و ماندگاری خوب هست من خیلی دوستش دارم'\n",
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "sent1_unigram = sentence1.split()\n",
    "sent1_bigram = get_bigram(normalizer.normalize(sentence1))\n",
    "sent1_trigram = get_trigram(normalizer.normalize(sentence1))\n",
    "sent1_unigram_prob = get_probability(sent1_unigram, unigram_prob)\n",
    "sent1_bigram_prob = get_probability(sent1_bigram, bigram_prob)\n",
    "sent1_trigram_prob = get_probability(sent1_trigram, trigram_prob)\n",
    "sent1_unigram_perplexity = get_perplexity(sent1_unigram, unigram_prob)\n",
    "sent1_bigram_perplexity = get_perplexity(sent1_bigram, bigram_prob)\n",
    "sent1_trigram_perplexity = get_perplexity(sent1_trigram, trigram_prob)\n",
    "\n",
    "\n",
    "sent2_unigram = sentence2.split()\n",
    "sent2_bigram = get_bigram(normalizer.normalize(sentence2))\n",
    "sent2_trigram = get_trigram(normalizer.normalize(sentence2))\n",
    "sent2_unigram_prob = get_probability(sent2_unigram, unigram_prob)\n",
    "sent2_bigram_prob = get_probability(sent2_bigram, bigram_prob)\n",
    "sent2_trigram_prob = get_probability(sent2_trigram, trigram_prob)\n",
    "sent2_unigram_perplexity = get_perplexity(sent2_unigram, unigram_prob)\n",
    "sent2_bigram_perplexity = get_perplexity(sent2_bigram, bigram_prob)\n",
    "sent2_trigram_perplexity = get_perplexity(sent2_trigram, trigram_prob)\n",
    "\n",
    "sent3_unigram = sentence3.split()\n",
    "sent3_bigram = get_bigram(normalizer.normalize(sentence3))\n",
    "sent3_trigram = get_trigram(normalizer.normalize(sentence3))\n",
    "sent3_unigram_prob = get_probability(sent3_unigram, unigram_prob)\n",
    "sent3_bigram_prob = get_probability(sent3_bigram, bigram_prob)\n",
    "sent3_trigram_prob = get_probability(sent3_trigram, trigram_prob)\n",
    "sent3_unigram_perplexity = get_perplexity(sent3_unigram, unigram_prob)\n",
    "sent3_bigram_perplexity = get_perplexity(sent3_bigram, bigram_prob)\n",
    "sent3_trigram_perplexity = get_perplexity(sent3_trigram, trigram_prob)\n",
    "\n",
    "sent4_unigram = sentence4.split()\n",
    "sent4_bigram = get_bigram(normalizer.normalize(sentence4))\n",
    "sent4_trigram = get_trigram(normalizer.normalize(sentence4))\n",
    "sent4_unigram_prob = get_probability(sent4_unigram, unigram_prob)\n",
    "sent4_bigram_prob = get_probability(sent4_bigram, bigram_prob)\n",
    "sent4_trigram_prob = get_probability(sent4_trigram, trigram_prob)\n",
    "sent4_unigram_perplexity = get_perplexity(sent4_unigram, unigram_prob)\n",
    "sent4_bigram_perplexity = get_perplexity(sent4_bigram, bigram_prob)\n",
    "sent4_trigram_perplexity = get_perplexity(sent4_trigram, trigram_prob)\n",
    "\n",
    "\n",
    "result_table = pd.DataFrame({'Unigram Probability':[sent1_unigram_prob, sent2_unigram_prob, sent3_unigram_prob, sent4_unigram_prob],\n",
    "                             'Bigram Probability': [sent1_bigram_prob, sent2_bigram_prob, sent3_bigram_prob, sent4_bigram_prob],\n",
    "                             'Trigram Probability': [sent1_trigram_prob, sent2_trigram_prob, sent3_trigram_prob, sent4_trigram_prob],\n",
    "                             'Unigram Perplexity': [sent1_unigram_perplexity, sent2_unigram_perplexity, sent3_unigram_perplexity, sent4_unigram_perplexity],\n",
    "                             'Bigram Perplexity': [sent1_bigram_perplexity, sent2_bigram_perplexity, sent3_bigram_perplexity, sent4_bigram_perplexity],\n",
    "                             'Trigram Perplexity': [sent1_trigram_perplexity, sent2_trigram_perplexity, sent3_trigram_perplexity, sent4_trigram_perplexity]},\n",
    "                            index=['sentence1', 'sentence2', 'sentence3', 'sentence4'])\n",
    "result_table"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence_trigram(first_sentence: str, input_sentence: list, ngram_probability: dict):\n",
    "\n",
    "    sentence_max_len = 12 - len(input_sentence)\n",
    "    predicted_aprt = []\n",
    "\n",
    "    for _ in range(sentence_max_len):\n",
    "        current_words = input_sentence[-1]\n",
    "\n",
    "        probable_words = {}\n",
    "        for tokens in ngram_probability.keys():\n",
    "            if tokens[0] == current_words[-1] and tokens[1] == current_words[-2] and tokens[2] == current_words[-3]:\n",
    "                probable_words[tokens] = ngram_probability[tokens]\n",
    "\n",
    "            if not probable_words:\n",
    "                if tokens[0] == current_words[-1] and tokens[1] == current_words[-2]:\n",
    "                    probable_words[tokens] = ngram_probability[tokens]\n",
    "\n",
    "\n",
    "            if not probable_words:\n",
    "                if tokens[0] == current_words[-1]:\n",
    "                    probable_words[tokens] = ngram_probability[tokens]\n",
    "\n",
    "        new_word = random.choices(list(probable_words.keys()), weights=probable_words.values())[0]\n",
    "        new_word = list(new_word)\n",
    "        predicted_aprt.append(new_word[0:])\n",
    "        input_sentence.append(new_word[0:])\n",
    "\n",
    "    index = 0\n",
    "    for np in predicted_aprt:\n",
    "        first_sentence  += ' '.join(np[1:]) + ' '\n",
    "        index += 1\n",
    "\n",
    "    return first_sentence, predicted_aprt\n",
    "\n",
    "\n",
    "def generate_sentence_bigram(first_sentence: str, input_sentence: list, ngram_probability: dict):\n",
    "\n",
    "    sentence_max_len = 12 - len(input_sentence)\n",
    "    predicted_aprt = []\n",
    "\n",
    "    for _ in range(sentence_max_len):\n",
    "        current_words = input_sentence[-1]\n",
    "\n",
    "        probable_words = {}\n",
    "        for tokens in ngram_probability.keys():\n",
    "            if tokens[0] == current_words[-1] and tokens[1] == current_words[-2]:\n",
    "                probable_words[tokens] = ngram_probability[tokens]\n",
    "\n",
    "            if not probable_words:\n",
    "                if tokens[0] == current_words[-1]:\n",
    "                    probable_words[tokens] = ngram_probability[tokens]\n",
    "\n",
    "        new_word = random.choices(list(probable_words.keys()), weights=probable_words.values())[0]\n",
    "        new_word = list(new_word)\n",
    "        predicted_aprt.append(new_word[0:])\n",
    "        input_sentence.append(new_word[0:])\n",
    "\n",
    "    index = 0\n",
    "    for np in predicted_aprt:\n",
    "        first_sentence  += ' '.join(np[1:]) + ' '\n",
    "        index += 1\n",
    "\n",
    "    return first_sentence, predicted_aprt"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence1 = 'کیفیت محصولات چینی زرین '\n",
    "sentence2 = 'از لحاظ جنس جنس خوبی داره '\n",
    "sentence3 = 'حتما پیشنهاد میکنم '\n",
    "sentence4 = 'بعد از چند روز استفاده '\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "sent1_bigram = get_bigram(normalizer.normalize(sentence1))\n",
    "sent1_trigram = get_trigram(normalizer.normalize(sentence1))\n",
    "\n",
    "sent2_bigram = get_bigram(normalizer.normalize(sentence2))\n",
    "sent2_trigram = get_trigram(normalizer.normalize(sentence2))\n",
    "\n",
    "sent3_bigram = get_bigram(normalizer.normalize(sentence3))\n",
    "sent3_trigram = get_trigram(normalizer.normalize(sentence3))\n",
    "\n",
    "sent4_bigram = get_bigram(normalizer.normalize(sentence4))\n",
    "sent4_trigram = get_trigram(normalizer.normalize(sentence4))\n",
    "\n",
    "\n",
    "full_sent1_bi, predicted_aprt1_bi = generate_sentence_bigram(sentence1, sent1_bigram, bigram_prob)\n",
    "full_sent2_bi, predicted_aprt2_bi = generate_sentence_bigram(sentence2, sent2_bigram, bigram_prob)\n",
    "full_sent3_bi, predicted_aprt3_bi = generate_sentence_bigram(sentence3, sent3_bigram, bigram_prob)\n",
    "full_sent4_bi, predicted_aprt4_bi = generate_sentence_bigram(sentence4, sent4_bigram, bigram_prob)\n",
    "\n",
    "full_sent1_bi_perplexity = get_perplexity(get_bigram(full_sent1_bi), bigram_prob)\n",
    "full_sent2_bi_perplexity = get_perplexity(get_bigram(full_sent2_bi), bigram_prob)\n",
    "full_sent3_bi_perplexity = get_perplexity(get_bigram(full_sent3_bi), bigram_prob)\n",
    "full_sent4_bi_perplexity = get_perplexity(get_bigram(full_sent4_bi), bigram_prob)\n",
    "\n",
    "full_sent1_tri, predicted_aprt1_tri = generate_sentence_trigram(sentence1, sent1_trigram, trigram_prob)\n",
    "full_sent2_tri, predicted_aprt2_tri = generate_sentence_trigram(sentence2, sent2_trigram, trigram_prob)\n",
    "full_sent3_tri, predicted_aprt3_tri = generate_sentence_trigram(sentence3, sent3_trigram, trigram_prob)\n",
    "full_sent4_tri, predicted_aprt4_tri = generate_sentence_trigram(sentence4, sent4_trigram, trigram_prob)\n",
    "\n",
    "full_sent1_tri_perplexity = get_perplexity(get_trigram(full_sent1_tri), trigram_prob)\n",
    "full_sent2_tri_perplexity = get_perplexity(get_trigram(full_sent2_tri), trigram_prob)\n",
    "full_sent3_tri_perplexity = get_perplexity(get_trigram(full_sent3_tri), trigram_prob)\n",
    "full_sent4_tri_perplexity = get_perplexity(get_trigram(full_sent4_tri), trigram_prob)\n",
    "\n",
    "\n",
    "\n",
    "predicted_sentence_res = pd.DataFrame({'sentence': [sentence1, sentence2, sentence3, sentence4],\n",
    "                                       'predicted with bigram':[full_sent1_bi, full_sent2_bi, full_sent3_bi, full_sent4_bi],\n",
    "                                       'predicted perplexity bigram': [full_sent1_bi_perplexity, full_sent2_bi_perplexity, full_sent3_bi_perplexity, full_sent4_bi_perplexity],\n",
    "                                       'predicted with trigram': [full_sent1_tri, full_sent2_tri, full_sent3_tri, full_sent4_tri],\n",
    "                                       'predicted perplexity trigram': [full_sent1_tri_perplexity, full_sent2_tri_perplexity, full_sent3_tri_perplexity, full_sent4_tri_perplexity]})\n",
    "\n",
    "predicted_sentence_res"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pos Tagging\n",
    "from hazm import POSTagger\n",
    "\n",
    "posTagger = POSTagger(model='pos_tagger.model')\n",
    "tagged_all_words = posTagger.tag(tokens = all_words)\n",
    "tagged_all_words"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count occurrence of all tokens\n",
    "\n",
    "def get_pos_occurrence(tagged_data: list):\n",
    "    pos_counter = {}  #key: pos  value: number of pos occurrence\n",
    "    for word, tag in tagged_data:\n",
    "        if tag not in pos_counter.keys():\n",
    "            pos_counter[tag] = 1\n",
    "        else:\n",
    "            pos_counter[tag] += 1\n",
    "    return pos_counter\n",
    "\n",
    "pos_counted = get_pos_occurrence(tagged_all_words)\n",
    "pos_counted_df = pd.DataFrame(data={'words': pos_counted.keys(), 'occurrence': pos_counted.values()})\n",
    "pos_counted_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# showing the most observed Nouns\n",
    "\n",
    "def get_name_occurrence(tagged_data: list):\n",
    "    name_counter = {}  #key: pos  value: number of pos occurrence\n",
    "    for word, tag in tagged_data:\n",
    "        if tag == 'NOUN':\n",
    "            if word not in name_counter.keys():\n",
    "                name_counter[word] = 0\n",
    "            name_counter[word] += 1\n",
    "    return name_counter\n",
    "\n",
    "name_counted = get_name_occurrence(tagged_all_words)\n",
    "name_counted_df = pd.DataFrame(data={'name': name_counted.keys(), 'occurrence': name_counted.values()}).sort_values(by='occurrence', ascending=False)[:15]\n",
    "name_counted_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
